{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "class NN(object):\n",
    "    \n",
    "    def __init__(self,hidden_dims=(1024,2048),n_hidden=2,mode='train',datapath=None,model_path=None):\n",
    "        self.init_method = ''\n",
    "        \n",
    "        self.h0 = 784     # +1 for bias\n",
    "        self.h1 = hidden_dims[0]\n",
    "        self.h2 = hidden_dims[1]\n",
    "        self.h3 = 10\n",
    "        \n",
    "        self.layer_sizes = [self.h0, self.h1, self.h2, self.h3]\n",
    "        self.network = [] # list of weights\n",
    "        self.num_hlayers = 2\n",
    "        self.num_classes = 10\n",
    "        \n",
    "        self.epochs = 10\n",
    "        self.loss = 0 \n",
    "        self.lr = 0.001\n",
    "        self.batch_size = 20\n",
    "        \n",
    "        self.dataprep()\n",
    "        self.initialize_weights()\n",
    "        self.train()\n",
    "#         self.test()\n",
    "        \n",
    "\n",
    "    def dataprep(self):\n",
    "\n",
    "        data = np.load('mnist.pkl.npy', encoding='latin1')\n",
    "\n",
    "        train_set = data[0][0]\n",
    "        val_set = data[1][0]\n",
    "        test_set = data[2][0]\n",
    "\n",
    "        self.val_labels = data[1][1]\n",
    "        self.test_labels = data[2][1]\n",
    "\n",
    "        self.train_size = train_set.shape[0]\n",
    "        self.val_size = val_set.shape[0]\n",
    "        self.test_size = test_set.shape[0]\n",
    "\n",
    "        p = np.random.permutation(self.train_size)\n",
    "        self.train_set = train_set[p]\n",
    "        self.train_labels = data[0][1][p]\n",
    "\n",
    "#         self.val_set = np.hstack((val_set, np.ones((self.val_size,1)) ))\n",
    "#         self.test_set = np.hstack((test_set, np.ones((self.test_size,1)) ))\n",
    "\n",
    "\n",
    "        print(train_set.shape)\n",
    "        print(val_set.shape)\n",
    "        print(test_set.shape)\n",
    "    \n",
    "    def train(self):    \n",
    "        m = self.train_size\n",
    "        val_samples = self.val_size\n",
    "\n",
    "        loss_history = np.zeros(self.epochs)\n",
    "        n_batches = int(m/self.batch_size)\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        for it in range(self.epochs):\n",
    "            loss = 0.0\n",
    "            val_loss= 0.0\n",
    "            indices = np.random.permutation(m)\n",
    "            train_set = self.train_set[indices]\n",
    "            train_labels = self.train_labels[indices]\n",
    "\n",
    "\n",
    "            for i in range(0,m,batch_size):\n",
    "                X_i = train_set[i:i+batch_size]\n",
    "                y_i = train_labels[i:i+batch_size]\n",
    "                outputs = self.forward(X_i)\n",
    "                loss_batch = self.backward(outputs, y_i)\n",
    "                loss += loss_batch\n",
    "\n",
    "            for i in range(0,val_samples,batch_size):\n",
    "                X_i_val = self.val_set[i:i+batch_size]\n",
    "                y_i_val = self.val_labels[i:i+batch_size]\n",
    "                outputs_val = self.forward(X_i_val)\n",
    "                loss_val_batch = loss(self, outputs_val, y_i_val)\n",
    "                val_loss += loss_val_batch\n",
    "\n",
    "\n",
    "\n",
    "            loss_history[it]  = loss\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        if(self.init_method=='zeros'):\n",
    "            for i in range(len(self.layer_sizes)-1):\n",
    "                layer_weights = np.zeros((self.layer_sizes[i]+1,self.layer_sizes[i+1]))\n",
    "                layer_weights[-1,:] = 0\n",
    "                print(layer_weights.shape)\n",
    "                self.network.append(layer_weights) \n",
    "        elif(self.init_method=='glorot'):\n",
    "\n",
    "            #compute the formula\n",
    "            for i in range(len(self.layer_sizes)-1):\n",
    "                d = sqrt(6.0/(self.layer_sizes[i]+self.layer_sizes[i+1]))      \n",
    "                layer_weights = np.random.uniform(-d,d,(self.layer_sizes[i]+1,self.layer_sizes[i+1]))\n",
    "                layer_weights[-1,:] = 0\n",
    "                print(layer_weights.shape)\n",
    "                self.network.append(layer_weights)    \n",
    "        else:\n",
    "            for i in range(len(self.layer_sizes)-1):\n",
    "                layer_weights = np.random.normal(0,1,(self.layer_sizes[i]+1,self.layer_sizes[i+1]))\n",
    "                layer_weights[-1,:] = 0  ##baises\n",
    "                print(layer_weights.shape)\n",
    "                self.network.append(layer_weights) \n",
    "        print('shapes of layers')\n",
    "        for net_wt in self.network:\n",
    "            print(net_wt.shape)\n",
    "  \n",
    "    def activation(self,inputs, layer_no):\n",
    "        print('inputs')\n",
    "        print(inputs.shape)\n",
    "        inputs = np.hstack((inputs, np.ones((inputs.shape[0],1)) ))\n",
    "        print('inputs shape post concat with 1s')\n",
    "        print(inputs.shape)\n",
    "        activation = np.dot(inputs, self.network[layer_no])     \n",
    "        return activation\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def softmax(self,inputs): \n",
    "        print('softmax inputs')\n",
    "        print(inputs.shape)\n",
    "        # Result of softmax are invariant even if we add/subtract a constant.    \n",
    "        ex = np.exp(inputs - np.max(inputs, axis=1,  keepdims=True))  # Subtract such that the maximum value is one.\n",
    "        return ex / ex.sum(axis=1,  keepdims=True)  \n",
    "\n",
    "    def forward(self,inputs):\n",
    "\n",
    "        #we are always appending inputs before doing relu to the cache\n",
    "\n",
    "        outputs = []\n",
    "        outputs.append(inputs)\n",
    "        print('forward loop inputs')\n",
    "        print(inputs.shape)\n",
    "        print('forward')\n",
    "        for layer_no in range(len(self.network)-1):        \n",
    "            inputs = self.activation(inputs, layer_no)\n",
    "            print('activated inputs')\n",
    "            print(inputs.shape)\n",
    "            outputs.append(inputs)\n",
    "            np.maximum(inputs,0,inputs)              #relu        \n",
    "            #outputs.append(inputs)\n",
    "\n",
    "        inputs = self.activation(inputs, len(self.network)-1 )\n",
    "        inputs = self.softmax(inputs)\n",
    "        outputs.append(inputs)\n",
    "\n",
    "        return outputs \n",
    "    \n",
    "    def relu(inputs):\n",
    "        outputs = np.maximum(inputs,0)\n",
    "        return outputs\n",
    "    \n",
    "    def backward(self,cache,labels):\n",
    "        #cache should have\n",
    "        #layer0_input    x0 \n",
    "        #layer1_output   x1=(w0(x0)+bo) (relu-ed in network but not here)\n",
    "        #layer2_output   x2=(w1(x1)+b1) (relu-ed in network but not here)\n",
    "        #network_outputs x3=softmax+crossentropy(w2(x2)+b2)    \n",
    "\n",
    "        predictions = cache[-1]\n",
    "        one_hot = np.zeros((self.batch_size, self.num_classes))\n",
    "        one_hot[np.arange(3), labels] = 1\n",
    "        loss = loss(self, predictions, one_hot)\n",
    "\n",
    "\n",
    "\n",
    "        #any avging div or normalisation for gradient sums? (for dot and sum)\n",
    "        #compute gradients for weights and biases\n",
    "        gradients = []\n",
    "        grad = predictions - one_hot /(1.0*self.batch_size)   ##minus here???\n",
    "\n",
    "\n",
    "        #transpose\n",
    "\n",
    "        #wrt w3 b3\n",
    "        temp = np.hstack((cache[-2], np.ones((predictions.shape[0],1)) ))\n",
    "\n",
    "        gradients.append(np.dot(np.transpose(temp),grad)) #weights        \n",
    "    #     gradients[1].append(grad) #bias\n",
    "        grad = grad * relu(self.network[2])\n",
    "        grad[temp] = 0 ##how to change this acc to temp\n",
    "\n",
    "        #wrt w2 b2\n",
    "        temp = np.hstack((cache[-3], np.ones((predictions.shape[0],1)) ))\n",
    "\n",
    "        gradients.append(np.dot(np.transpose(temp),grad)) #weights    \n",
    "    #     gradients[1].append(grad) #bias\n",
    "        grad = grad * relu(self.network[1])\n",
    "        grad[temp] = 0 \n",
    "\n",
    "        #wrt w1 b1\n",
    "        temp = np.hstack((cache[-4], np.ones((predictions.shape[0],1)) ))\n",
    "\n",
    "        gradients.append(np.dot(np.transpose(temp),grad)) #weights\n",
    "    #     gradients[1].append(grad) #bias\n",
    "        grad = grad * relu(self.network[0])\n",
    "        grad[temp] = 0     \n",
    "\n",
    "        self.update(gradients)\n",
    "\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def reluDerivative(x):  #not needed right? and diff formula here is okay?\n",
    "         x[x<=0] = 0\n",
    "         x[x>0] = 1\n",
    "         return x\n",
    "        \n",
    "    def loss(self,prediction,labels): #cross entropy\n",
    "\n",
    "\n",
    "        epsilon = 1e-12\n",
    "        predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "        ce = -np.sum(np.multiply(targets,np.log(predictions+1e-9)), axis=1)/(labels.shape[0]*1.0)\n",
    "    #     loss = np.sum(-one_hot_labels * np.log(ao))\n",
    "        return loss\n",
    "    \n",
    "    def update(self,grads):\n",
    "        length_net = len(self.network)\n",
    "        assert(len(grads)==length_net)\n",
    "\n",
    "        i=0\n",
    "        for layer_no in range(length_net):\n",
    "            self.network[layer_no] += grads[length_net-1-i]             \n",
    "            i += 1\n",
    "            \n",
    "    def test(self):\n",
    "        m = len(self.test_size)\n",
    "        loss_history = np.zeros(self.epochs)\n",
    "        n_batches = int(m/self.batch_size)\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        for it in range(self.epochs):\n",
    "            loss =0.0\n",
    "            indices = np.random.permutation(m)\n",
    "            self.test_set = self.test_set[indices]\n",
    "            self.test_labels = self.test_labels[indices]\n",
    "            for i in range(0,m,batch_size):\n",
    "                X_i = self.test_set[i:i+batch_size]\n",
    "                y_i = self.test_labels[i:i+batch_size]\n",
    "                outputs = self.forward(self, X_i)\n",
    "\n",
    "                #need loss and predictions\n",
    "\n",
    "            loss_history[it]  = loss\n",
    "\n",
    "        return loss_history\n",
    "    \n",
    "nn = NN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
