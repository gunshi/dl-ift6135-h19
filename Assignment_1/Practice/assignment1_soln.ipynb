{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mnist.pkl.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e5c988be6db8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-e5c988be6db8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hidden_dims, n_hidden, mode, datapath, model_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataprep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-e5c988be6db8>\u001b[0m in \u001b[0;36mdataprep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdataprep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist.pkl.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mnist.pkl.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "class NN(object):\n",
    "    \n",
    "    def __init__(self,hidden_dims=(1024,2048),n_hidden=2,mode='train',datapath=None,model_path=None):\n",
    "        self.init_method = 'glorot'\n",
    "        \n",
    "        self.h0 = 784     # +1 for bias\n",
    "        self.h1 = hidden_dims[0]\n",
    "        self.h2 = hidden_dims[1]\n",
    "        self.h3 = 10\n",
    "        \n",
    "        self.layer_sizes = [self.h0, self.h1, self.h2, self.h3]\n",
    "        self.network = [] # list of weights\n",
    "        self.num_hlayers = 2\n",
    "        self.num_classes = 10\n",
    "        \n",
    "        self.epochs = 10\n",
    "        self.lr = 0.0001\n",
    "        self.batch_size = 40\n",
    "        \n",
    "        self.dataprep()\n",
    "        self.initialize_weights()\n",
    "        self.train()\n",
    "        self.test()\n",
    "        \n",
    "\n",
    "    def dataprep(self):\n",
    "\n",
    "        data = np.load('/content/mnist.pkl.npy', encoding='latin1')\n",
    "\n",
    "        train_set = data[0][0]\n",
    "        self.val_set = data[1][0]\n",
    "        self.test_set = data[2][0]\n",
    "\n",
    "        self.val_labels = data[1][1]\n",
    "        self.test_labels = data[2][1]\n",
    "\n",
    "        self.train_size = train_set.shape[0]\n",
    "        self.val_size = self.val_set.shape[0]\n",
    "        self.test_size = self.test_set.shape[0]\n",
    "\n",
    "        p = np.random.permutation(self.train_size)\n",
    "        self.train_set = train_set[p]\n",
    "        self.train_labels = data[0][1][p]\n",
    "\n",
    "        print(self.train_set.shape)\n",
    "        print(self.val_set.shape)\n",
    "        print(self.test_set.shape)\n",
    "    \n",
    "    def train(self):    \n",
    "        m = self.train_size\n",
    "        val_samples = self.val_size\n",
    "\n",
    "        loss_history = np.zeros(self.epochs)\n",
    "        n_batches = int(m/self.batch_size)\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        \n",
    "        \n",
    "        print('zero shot validation')\n",
    "        val_batches = 0\n",
    "        val_loss= 0.0\n",
    "        val_accuracy = 0\n",
    "        for i in range(0,val_samples-batch_size+1,batch_size):\n",
    "            X_i_val = self.val_set[i:i+batch_size]\n",
    "            y_i_val = self.val_labels[i:i+batch_size]\n",
    "            outputs_val = self.forward(X_i_val)\n",
    "            \n",
    "            labels = np.argmax(outputs_val[-1], axis=1)\n",
    "            val_accuracy += (y_i_val == labels).sum()           \n",
    "            \n",
    "            one_hot = np.zeros((self.batch_size, self.num_classes))\n",
    "            one_hot[np.arange(self.batch_size), y_i_val] = 1\n",
    "            loss_val_batch = self.loss(outputs_val[-1], one_hot)\n",
    "            val_loss += loss_val_batch\n",
    "            val_batches +=1\n",
    "        val_accuracy = val_accuracy/(1.0*val_batches*self.batch_size)\n",
    "        val_loss = val_loss/(1.0*val_batches*self.batch_size)\n",
    "        print('val accuracy : '+str(val_accuracy))\n",
    "\n",
    "        print('val loss : ' + str(val_loss))\n",
    "            \n",
    "            \n",
    "        \n",
    "        for it in range(self.epochs):\n",
    "            print('epoch : '+str(it))\n",
    "            loss = 0.0\n",
    "            val_loss= 0.0\n",
    "            train_accuracy = 0\n",
    "            indices = np.random.permutation(m)\n",
    "            train_set = self.train_set[indices]\n",
    "            train_labels = self.train_labels[indices]\n",
    "\n",
    "\n",
    "            train_batches = 0 \n",
    "            for i in range(0,m-batch_size+1,batch_size):\n",
    "                X_i = train_set[i:i+batch_size]\n",
    "                y_i = train_labels[i:i+batch_size]\n",
    "                outputs = self.forward(X_i)\n",
    "                loss_batch, accuracy_batch = self.backward(outputs, y_i)\n",
    "                loss += loss_batch\n",
    "                train_accuracy += accuracy_batch\n",
    "                train_batches +=1\n",
    "            loss = loss/(1.0*train_batches*self.batch_size)\n",
    "            train_accuracy = train_accuracy/(1.0*train_batches*self.batch_size)\n",
    "\n",
    "            print('train loss : ' + str(loss))      \n",
    "            print('train accuracy : '+ str(train_accuracy))\n",
    "\n",
    "            print('validation')\n",
    "            val_batches = 0\n",
    "            val_accuracy = 0\n",
    "            for i in range(0,val_samples-batch_size+1,batch_size):\n",
    "           \n",
    "        \n",
    "                X_i_val = self.val_set[i:i+batch_size]\n",
    "                y_i_val = self.val_labels[i:i+batch_size]\n",
    "                outputs_val = self.forward(X_i_val)\n",
    "                labels = np.argmax(outputs_val[-1], axis=1)\n",
    "                val_accuracy += (y_i_val == labels).sum()  \n",
    "            \n",
    "                one_hot = np.zeros((self.batch_size, self.num_classes))\n",
    "                one_hot[np.arange(self.batch_size), y_i_val] = 1\n",
    "                                \n",
    "                loss_val_batch = self.loss(outputs_val[-1], one_hot)\n",
    "                val_loss += loss_val_batch\n",
    "                val_batches +=1\n",
    "            val_loss = val_loss/(1.0*val_batches*self.batch_size)\n",
    "            val_accuracy = val_accuracy/(1.0*val_batches*self.batch_size)\n",
    "            print('val accuracy : '+str(val_accuracy))\n",
    "            print('val loss : ' + str(val_loss))\n",
    "\n",
    "\n",
    "\n",
    "            loss_history[it]  = loss\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        if(self.init_method=='zeros'):\n",
    "            for i in range(len(self.layer_sizes)-1):\n",
    "                layer_weights = np.zeros((self.layer_sizes[i]+1,self.layer_sizes[i+1]))\n",
    "                layer_weights[-1,:] = 0\n",
    "                print(layer_weights.shape)\n",
    "                self.network.append(layer_weights) \n",
    "        elif(self.init_method=='glorot'):\n",
    "\n",
    "            #compute the formula\n",
    "            for i in range(len(self.layer_sizes)-1):\n",
    "                d = math.sqrt(6.0/(self.layer_sizes[i]+self.layer_sizes[i+1]))      \n",
    "                layer_weights = np.random.uniform(-d,d,(self.layer_sizes[i]+1,self.layer_sizes[i+1]))\n",
    "                layer_weights[-1,:] = 0\n",
    "                print(layer_weights.shape)\n",
    "                self.network.append(layer_weights)    \n",
    "        else:\n",
    "            for i in range(len(self.layer_sizes)-1):\n",
    "                layer_weights = np.random.normal(0,1,(self.layer_sizes[i]+1,self.layer_sizes[i+1]))\n",
    "                layer_weights[-1,:] = 0  ##baises\n",
    "                print(layer_weights.shape)\n",
    "                self.network.append(layer_weights) \n",
    "\n",
    "  \n",
    "    def activation(self,inputs, layer_no):\n",
    "        inputs = np.hstack((inputs, np.ones((inputs.shape[0],1)) ))\n",
    "        activation = np.dot(inputs, self.network[layer_no])     \n",
    "        return activation\n",
    "    \n",
    "    def softmax(self,inputs): \n",
    "        # Result of softmax are invariant even if we add/subtract a constant.  \n",
    " \n",
    "        ex = np.exp(inputs - np.max(inputs, axis=1,  keepdims=True))  # Subtract such that the maximum value is one.\n",
    "\n",
    "        return ex / ex.sum(axis=1,  keepdims=True)  \n",
    "\n",
    "    def forward(self,inputs):\n",
    "\n",
    "        #we are always appending inputs before doing relu to the cache\n",
    "\n",
    "        outputs = []\n",
    "        outputs.append(inputs)\n",
    "\n",
    "        for layer_no in range(len(self.network)-1):        \n",
    "            inputs = self.activation(inputs, layer_no)\n",
    "\n",
    "            outputs.append(inputs)\n",
    "            np.maximum(inputs,0,inputs)              #relu     \n",
    "       \n",
    "        inputs = self.activation(inputs, len(self.network)-1 )\n",
    "\n",
    "        inputs = self.softmax(inputs)\n",
    "\n",
    "        outputs.append(inputs)\n",
    "\n",
    "        return outputs \n",
    "    \n",
    "    def relu(self, inputs):\n",
    "        outputs = np.maximum(inputs,0)\n",
    "        return outputs\n",
    "    \n",
    "    def backward(self,cache,labels):\n",
    "        #cache should have\n",
    "        #layer0_input    x0 \n",
    "        #layer1_output   x1=(w0(x0)+bo) (relu-ed in network but not here)\n",
    "        #layer2_output   x2=(w1(x1)+b1) (relu-ed in network but not here)\n",
    "        #network_outputs x3=softmax+crossentropy(w2(x2)+b2)    \n",
    "\n",
    "        predictions = cache[-1]\n",
    "        one_hot = np.zeros((self.batch_size, self.num_classes))\n",
    "        one_hot[np.arange(self.batch_size), labels] = 1\n",
    "        loss_batch = self.loss(predictions, one_hot)\n",
    "\n",
    "        pred_labels = np.argmax(predictions, axis=1)\n",
    "        accuracy_batch = (labels == pred_labels).sum()        \n",
    "\n",
    "        #any avging div or normalisation for gradient sums? (for dot and sum)\n",
    "        #compute gradients for weights and biases\n",
    "        gradients = []\n",
    "        grad =  (one_hot - predictions )  /(1.0*self.batch_size)   \n",
    "\n",
    "\n",
    "        #wrt w3 b3\n",
    "        temp = np.hstack((cache[-2], np.ones((predictions.shape[0],1)) ))\n",
    "\n",
    "\n",
    "        gradients.append(np.dot(np.transpose(self.relu(temp)),grad)) #weights        \n",
    "        grad = np.dot(grad, np.transpose(self.network[2][:-1,:]))   \n",
    "        grad[cache[-2]<0] = 0 \n",
    "\n",
    "        #wrt w2 b2\n",
    "        temp = np.hstack((cache[-3], np.ones((predictions.shape[0],1)) ))\n",
    "        gradients.append(np.dot(np.transpose(self.relu(temp)),grad)) #weights    \n",
    "        grad = np.dot(grad, np.transpose(self.network[1][:-1,:]))\n",
    "\n",
    "        grad[cache[-3]<0] = 0 \n",
    "\n",
    "        #wrt w1 b1\n",
    "        temp = np.hstack((cache[-4], np.ones((predictions.shape[0],1)) )) \n",
    "        gradients.append(np.dot(np.transpose(temp),grad)) #weights\n",
    "\n",
    "        self.update(gradients)\n",
    "\n",
    "        return loss_batch, accuracy_batch\n",
    "\n",
    "    def reluDerivative(x): \n",
    "        x[x<=0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "        \n",
    "        \n",
    "    def loss(self,predictions,targets): #cross entropy\n",
    "\n",
    "        epsilon = 1e-12\n",
    "#         predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "\n",
    "        ce = -np.sum(np.multiply(targets,np.log(predictions+1e-9)), axis=1)/(targets.shape[0]*1.0)\n",
    "\n",
    "        ce = np.sum(ce)\n",
    "        return ce\n",
    "    \n",
    "    def update(self,grads):\n",
    "        length_net = len(self.network)\n",
    "        assert(len(grads)==length_net)\n",
    "\n",
    "        i=0\n",
    "        for layer_no in range(length_net):\n",
    "            self.network[layer_no] += self.lr * grads[length_net-1-i]             \n",
    "            i += 1\n",
    "            \n",
    "    def test(self):\n",
    "        m = len(self.test_size)\n",
    "        loss_history = np.zeros(self.epochs)\n",
    "        n_batches = int(m/self.batch_size)\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        for it in range(self.epochs):\n",
    "            loss =0.0\n",
    "            indices = np.random.permutation(m)\n",
    "            self.test_set = self.test_set[indices]\n",
    "            self.test_labels = self.test_labels[indices]\n",
    "            for i in range(0,m,batch_size):\n",
    "                X_i = self.test_set[i:i+batch_size]\n",
    "                y_i = self.test_labels[i:i+batch_size]\n",
    "                \n",
    "                \n",
    "                outputs = self.forward(X_i)\n",
    "                \n",
    "                one_hot = np.zeros((self.batch_size, self.num_classes))\n",
    "                one_hot[np.arange(self.batch_size), y_i] = 1                \n",
    "                loss += self.loss(outputs[-1], one_hot)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "nn = NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
